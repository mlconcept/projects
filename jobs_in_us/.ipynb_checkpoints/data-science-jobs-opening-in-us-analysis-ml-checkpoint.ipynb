{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": false,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Import all the required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Load CSV file into a DataFrame\n",
    "raw_data=pd.read_csv('../input/alldata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55aad66783327c2e4a858776b5a5310f8a24c02f"
   },
   "outputs": [],
   "source": [
    "#PART 1, Data Analysis\n",
    "#Now, we are finding total number of compnaies who require data scientist\n",
    "total_no_company=raw_data['company'].nunique()\n",
    "print('Toatl number of firms with data science job vacancies',total_no_company)\n",
    "\n",
    "#finding highest number of vacancy in a company\n",
    "most_vacancy= raw_data.groupby(['company'])['position'].count()\n",
    "most_vacancy=most_vacancy.reset_index(name='position')\n",
    "most_vacancy=most_vacancy.sort_values(['position'],ascending=False)\n",
    "pareto_df=most_vacancy\n",
    "most_vacancy=most_vacancy.head(25)\n",
    "print('Top 10 firms with most vacancies',most_vacancy)\n",
    "\n",
    "# Plot graph for top most vacancy\n",
    "fig, ax = plt.subplots(figsize = (10,6))\n",
    "ax=seaborn.barplot(x=\"company\", y=\"position\", data=most_vacancy)    \n",
    "ax.set_xticklabels(most_vacancy['company'],rotation=90)  \n",
    "ax.set_xlabel('COMPANY WITH MOST JOBS',fontsize=16, color='red')\n",
    "ax.set_ylabel('# OF JOBS',fontsize=16,color='red') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2bda66d92cd5ea2b3b419aa3ee7a3a051d5d145e"
   },
   "outputs": [],
   "source": [
    "# Finding total number of unique roles in data science domain from the given dataset\n",
    "total_no_roles=raw_data['position'].nunique()\n",
    "print('Toatl number of roles across all the firms',total_no_roles)\n",
    "\n",
    "# most offered roles across all the firms\n",
    "most_offd_roles=raw_data.groupby(['position'])['company'].count()   \n",
    "most_offd_roles=most_offd_roles.reset_index(name='company')\n",
    "most_offd_roles=most_offd_roles.sort_values(['company'],ascending=False)\n",
    "most_offd_roles=most_offd_roles.head(30)   \n",
    "print('Top 15 most wanted roles across firms',most_offd_roles)\n",
    "\n",
    "# Plot graph for top most offered roles\n",
    "fig,ax=plt.subplots(figsize=(12,6))\n",
    "ax=seaborn.barplot(x=\"position\", y=\"company\", data=most_offd_roles)    \n",
    "ax.set_xticklabels(most_offd_roles['position'],rotation=90)\n",
    "ax.set_xlabel('MOST WANTED JOB ROLES',fontsize=20,color='red')\n",
    "ax.set_ylabel('NO OF ROLES ACROSS INDUSTRY',fontsize=16,color='red')#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eacd9876958647a8c028fc6c2b19487172e7f9a6"
   },
   "outputs": [],
   "source": [
    "# Finding total number of cities with Data science jobs\n",
    "total_no_cities=raw_data['location'].nunique()\n",
    "\n",
    "#cities and total no of openings w.r.t companies\n",
    "city_and_roles=raw_data.groupby(['location','company'])['position'].count()     \n",
    "city_and_roles=city_and_roles.reset_index()\n",
    "city_and_roles=city_and_roles.sort_values(['position'],ascending=False)\n",
    "city_and_roles=city_and_roles.head(15) \n",
    "\n",
    "# Plot graph for top most cities and no of roles\n",
    "fig,a=plt.subplots(figsize=(10,6))             \n",
    "a=seaborn.barplot(x=\"company\", y=\"position\", hue=\"location\", data=city_and_roles);    \n",
    "a.set_xticklabels(city_and_roles['company'],rotation=90)   \n",
    "a.set_ylabel('No Of Positions',fontsize=16,color='red')\n",
    "a.set_xlabel('Company Name',fontsize=16,color='red')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a157096ffc1d4b1217b4ef844cad2ed578f50c5"
   },
   "outputs": [],
   "source": [
    "#PART 2, PARETO CHART\n",
    "# trying to find if the given data set follows Pareto(80,20) rule\n",
    "\n",
    "#find the total job openings\n",
    "total_job_openings=len(raw_data['position'])\n",
    "\n",
    "#find 70% total job (total_job_openings) openings, pareto rule,can be (80,20) or (70,20)\n",
    "sum_70_percent_job_openings=total_job_openings/100*70\n",
    "\n",
    "#find 20% total number of companies\n",
    "sum_20_percent_companies=total_no_company/100*20\n",
    "\n",
    "#now find the total number of job openings from those 20% top comapnies \n",
    "top_20_companies_job_openings=pareto_df.head(443)\n",
    "\n",
    "sum_=top_20_companies_job_openings['position'].sum()\n",
    "\n",
    "print('70% of the total job openings is :',sum_70_percent_job_openings )\n",
    "\n",
    "print('total job openings from top 20% of the companies :',sum_)\n",
    "\n",
    "print ('so, 70% of the total job openings and total job openings from top 20% of the companies are almost equal. Therefore we can say the data set follows Pareto Rule')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3b761263cae914834994488fedc161c9d125334"
   },
   "outputs": [],
   "source": [
    "#PART 3 MACHINE LEARNING\n",
    "#there are so many job profiles in teh given dataset so lets Categories them into 5; Data Scientist, Machine Learning Engineer, Data Analyst, Data Science Manager and Others\n",
    "\n",
    "# Creating only 5 datascience roles among all\n",
    "data=raw_data.copy()\n",
    "data.dropna(subset=['position'], how='all', inplace = True)\n",
    "data['position']=[x.upper() for x in data['position']]\n",
    "data['description']=[x.upper() for x in data['description']]\n",
    "\n",
    "data.loc[data.position.str.contains(\"SCIENTIST\"), 'position'] = 'Data Scientist'\n",
    "\n",
    "data.loc[data.position.str.contains('ENGINEER'),'position']='Machine Learning Engineer'\n",
    "data.loc[data.position.str.contains('PRINCIPAL STATISTICAL PROGRAMMER'),'position']='Machine Learning Engineer'\n",
    "data.loc[data.position.str.contains('PROGRAMMER'),'position']='Machine Learning Engineer'\n",
    "data.loc[data.position.str.contains('DEVELOPER'),'position']='Machine Learning Engineer'\n",
    "\n",
    "data.loc[data.position.str.contains('ANALYST'), 'position'] = 'Data Analyst'\n",
    "data.loc[data.position.str.contains('STATISTICIAN'), 'position'] = 'Data Analyst'\n",
    "\n",
    "data.loc[data.position.str.contains('MANAGER'),'position']='Data Science Manager'\n",
    "data.loc[data.position.str.contains('CONSULTANT'),'position']='Data Science Manager'\n",
    "data.loc[data.position.str.contains('DATA SCIENCE'),'position']='Data Science Manager'\n",
    "data.loc[data.position.str.contains('DIRECTOR'),'position']='Data Science Manager'\n",
    "\n",
    "data.position=data[(data.position == 'Data Scientist') | (data.position == 'Data Analyst') | (data.position == 'Machine Learning Engineer') | (data.position == 'Data Science Manager')]\n",
    "data.position=['Others' if x is np.nan else x for x in data.position]\n",
    "\n",
    "position=data.groupby(['position'])['company'].count()   \n",
    "position=position.reset_index(name='company')\n",
    "position=position.sort_values(['company'],ascending=False)\n",
    "\n",
    "print('Here is  the count of each new roles we created :', '\\n\\n', position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19e438bdf4e1afb75b014eea0b0aa9f801d0695f"
   },
   "outputs": [],
   "source": [
    "# Next Part in ML Algorithm is Data Cleansing\n",
    "X=data.description\n",
    "Y=data.position\n",
    "\n",
    "X=[re.sub(r\"[^a-zA-Z0-9]+\", ' ', k) for k in X]\n",
    "X=[re.sub(\"[0-9]+\",' ',k) for k in X]\n",
    "\n",
    "#applying stemmer\n",
    "ps =PorterStemmer()\n",
    "X=[ps.stem(k) for k in X]\n",
    "\n",
    "#Note: I have not removed stop words because there are important key words mentioned in job description which are of length 2, I feel they have weightage while classifing\n",
    "tfidf=TfidfVectorizer()\n",
    "label_enc=LabelEncoder()\n",
    "\n",
    "X=tfidf.fit_transform(X)\n",
    "Y=label_enc.fit_transform(Y)\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(X,Y,stratify=Y,test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48a174867930c8879154dbc63e130865ceb68a00"
   },
   "outputs": [],
   "source": [
    "# first algorithm SVM\n",
    "#SVM classification\n",
    "#svm=SVC(kernel='rbf')\n",
    "#svm.fit(x_train,y_train)\n",
    "\n",
    "#svm_y=svm.predict(x_test)\n",
    "\n",
    "#print('Accuracy of SVM :', accuracy_score(y_test,svm_y))\n",
    "#print ('Confusion Matrix of SVM : ', '\\n\\n', confusion_matrix(y_test,svm_y))\n",
    "\n",
    "#crossfold Validation of 7 folds for SVM\n",
    "#cross_val_SVM=sklearn.model_selection.cross_validate(svm, x_train, y=y_train,cv=7)\n",
    "\n",
    "#print ('SVM Train fit score is : ', '\\n\\n', cross_val_SVM ['train_score'])\n",
    "#print ('SVM TEST score is : ', '\\n\\n', cross_val_SVM ['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "916dd4f0fa65d6f90ed4cadc050f6fb4c995f199"
   },
   "outputs": [],
   "source": [
    "#Naive Bayes classification\n",
    "NB=MultinomialNB()\n",
    "NB.fit(x_train,y_train)\n",
    "NB_y=NB.predict(x_test)\n",
    "\n",
    "print('Accuracy of NB :', accuracy_score(y_test,NB_y))\n",
    "print ('Confusion Matrix of NB : ', '\\n\\n', confusion_matrix(y_test,NB_y))\n",
    "\n",
    "#crossfold Validation of 7 folds for NB\n",
    "cross_val_NB=sklearn.model_selection.cross_validate(NB, x_train, y=y_train,cv=7)\n",
    "\n",
    "print ('NB Train fit score is : ', '\\n\\n', cross_val_NB ['train_score'])\n",
    "print ('NB TEST score is : ', '\\n\\n', cross_val_NB ['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "71ca6c39b172b3804038ee192d74e3bc6b9d2640"
   },
   "outputs": [],
   "source": [
    "#3rd Classifier SGDC\n",
    "#SGD classification\n",
    "sgd=SGDClassifier()\n",
    "sgd.fit(x_train,y_train)\n",
    "sgd_y=sgd.predict(x_test)\n",
    "\n",
    "print('Accuracy of SGD :', accuracy_score(y_test,sgd_y))\n",
    "print ('Confusion Matrix of SGD : ', '\\n\\n', confusion_matrix(y_test,sgd_y))\n",
    "\n",
    "#crossfold Validation of 7 folds for SGD\n",
    "cross_val_SGD=sklearn.model_selection.cross_validate(sgd, x_train, y=y_train,cv=7)\n",
    "\n",
    "print ('SGD Train fit score is : ', '\\n\\n', cross_val_SGD ['train_score'])\n",
    "print ('SGD TEST score is : ', '\\n\\n', cross_val_SGD ['test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "62708026be1b43fd9d86f959d48866bfe84df6e8"
   },
   "outputs": [],
   "source": [
    "#4th Classifier \n",
    "#XGBOOST classification\n",
    "#xgboost=GradientBoostingClassifier(n_estimators=90)\n",
    "#xgboost.fit(x_train,y_train)\n",
    "#xgboost_y=xgboost.predict(x_test)\n",
    "\n",
    "#print('Accuracy of XGBOOST :', accuracy_score(y_test,xgboost_y))\n",
    "#print ('Confusion Matrix of XGBOOST : ', '\\n\\n', confusion_matrix(y_test,xgboost_y))\n",
    "\n",
    "#crossfold Validation of 7 folds for SGD\n",
    "#cross_val_xgboost=sklearn.model_selection.cross_validate(xgboost, x_train, y=y_train,cv=7)\n",
    "\n",
    "#print ('XGBOOST Train fit score is : ', '\\n\\n', cross_val_xgboost ['train_score'])\n",
    "#print ('XGBOOST TEST score is : ', '\\n\\n', cross_val_xgboost ['test_score']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3c6f5217c34520d875685e34583d62996c6d2f14"
   },
   "outputs": [],
   "source": [
    "# Inverse Transform of label Encoder\n",
    "print (label_enc.inverse_transform([0,1,2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3f950b91bcf90670d2d0dad7fecd179fd637f00"
   },
   "outputs": [],
   "source": [
    "\"\"\"END NOTES\n",
    "\n",
    "•\tGraphs can be improved, these are designed for beginner purpose\n",
    "\n",
    "•\tPareto graph is not plotted since I had issues with my Work laptop to install certain packages, but I have given the numbers which means mostly the same\n",
    "\n",
    "•\tML can be improved with the following ideas;\n",
    "\n",
    "        Categories the data in a better way, so that they are much meaningful\n",
    "        Balance the categories so that we will not have any skewness as we have here\n",
    "        Do not run SVM or XGBOOST if you do  not have an hour of patience\n",
    "        Rather run Naïve Bayes and SGDC which are almost performing same as SVM and XGBOOST respectively\n",
    "        The fit is not good in SVM & Naïve Bayes because the categories (Job Description) are very closely correlated and may be due to biased data \n",
    "        Look at the confusion matrix of SVM and NB, clearly due to biase the data is pulled towards DATA SCIENTIST class\n",
    "        I have tried to change kernel and otehr parameters for SVM & NB but no improvements\n",
    "        XGBOOST & SGDC are doing good with 79-80% accuracy consistently but we should look at the confusion matrix and try to improve recall and precision\n",
    "        \n",
    "        \n",
    "•\tPlease drop a note if you feel I can do any better work \n",
    "\n",
    "You can contact me on\n",
    "mmpramod7@gmail.com or\n",
    "Pramod Manjegowda @ linkedIn \n",
    "\n",
    "\n",
    "THANKS A TON \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "043f6bb67a44fbefc544c25dcd253c5f01faf90b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
